{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([1.], device='mps:0')"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "assert torch.backends.mps.is_available()\n",
                "mps_device = torch.device(\"mps\")\n",
                "torch.ones(1, device=mps_device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'Long before elves came to this continent, there was a great war between the elves and the humans. The elves were the first to fight the humans, and the humans were the first to defeat the elves. The elves were the first to defeat the humans'"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt = \"Long before elves came to this continent, there was a\"\n",
                "tokenizer.batch_decode(model.generate(tokenizer(prompt, return_tensors=\"pt\").input_ids, max_length=50))[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Library/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
                        "  warnings.warn(\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'Hello, I am a little bit of a fan of the original series. I have been a fan of the original series for a long time, and I have been a fan of the original series for a long time. I have been a fan of'"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt = \"Hello, I am\"\n",
                "tokenizer.batch_decode(model.generate(tokenizer(prompt, return_tensors=\"pt\").input_ids, max_length=50, temperature=0.0))[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}