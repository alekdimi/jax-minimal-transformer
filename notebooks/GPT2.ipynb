{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 1\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "import flax.linen as nn\n",
                "import jax.numpy as jnp\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\" )))\n",
                "\n",
                "%aimport gpt.model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n",
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "W0000 00:00:1721355796.356624 8112131 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
                        "I0000 00:00:1721355796.366036 8112131 service.cc:145] XLA service 0x16d05f2a0 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
                        "I0000 00:00:1721355796.366044 8112131 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
                        "I0000 00:00:1721355796.367144 8112131 mps_client.cc:406] Using Simple allocator.\n",
                        "I0000 00:00:1721355796.367151 8112131 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Metal device set to: Apple M2\n",
                        "\n",
                        "systemMemory: 16.00 GB\n",
                        "maxCacheSize: 5.33 GB\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "gpt2 = gpt.model.GPT.from_pretrained()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Long before elves came to this continent, there was a great war between the elves and the humans. The elves were the first to fight the humans, and the humans were the first to defeat the elves. The elves were the first to defeat the humans\n"
                    ]
                }
            ],
            "source": [
                "# prompt = \"Harry Potter's fathers name is\" # came to this continent, there was a\"\n",
                "# prompt = \"Hello world\"\n",
                "# prompt = \"Q: What is the tallest mountain in the world? A:\"\n",
                "prompt = \"Long before elves came to this continent, there was a\"\n",
                "\n",
                "output = gpt2.generate(\n",
                "    prompt, kv_cache=True, streaming=True, max_tokens=50, output_logits=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Long before elves came to this continent, there was a great war between the elves and the humans. The elves were the first to fight the humans, and the humans were the first to defeat the elves. The elves were the first to defeat the humans\n"
                    ]
                }
            ],
            "source": [
                "output_no_cache = gpt2.generate(\n",
                "    prompt, kv_cache=False, streaming=True, max_tokens=50, output_logits=True\n",
                ")\n",
                "assert jnp.all(output.tokens == output_no_cache.tokens)\n",
                "assert output.logits is not None and output_no_cache.logits is not None\n",
                "assert jnp.allclose(output.logits, output_no_cache.logits)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
